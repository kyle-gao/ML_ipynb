{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF_Transformer_Calculus.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNBuyT5u8Itcb+8ow/f/4Cd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyle-gao/ML_ipynb/blob/unfinished/TF_Transformer_Calculus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anQuvdlJJ8_r",
        "colab_type": "text"
      },
      "source": [
        "Copyright 2020 Yi Lin(Kyle) Gao\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_qNSzzyaCbD",
        "colab_type": "text"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "jmjh290raIky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbRTcwfC7JVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import time"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlMtl_Tr8l44",
        "colab_type": "text"
      },
      "source": [
        "# Training a transformer for symbolic mathematics (differentiation) following the Transfomer tutorial https://www.tensorflow.org/tutorials/text/transformer\n",
        "The dataset used is https://github.com/deepmind/mathematics_dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_LQw06j-KLN",
        "colab_type": "text"
      },
      "source": [
        "# Data pipeline with tfds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUctAmGT7-C2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, val = tfds.load(\n",
        "    'math_dataset/calculus__differentiate',\n",
        "    split=['train[:200000]', 'test[:5000]'],\n",
        "    as_supervised=True)"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ba5HuWnRBbXA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#since we are dealing with mathemetics, we expect a low vocabulary size\n",
        "tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    (question.numpy() for question, answer in train), target_vocab_size=2**12)"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2lcMhDrAraT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7b40937f-2ac2-4f48-a6be-1b1eb43034aa"
      },
      "source": [
        "#Taking a look at the dataset\n",
        "test = list(train.take(1))\n",
        "test"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(<tf.Tensor: shape=(), dtype=string, numpy=b'Find the first derivative of -4*a**4*v - 84*a**4 - v - 226 wrt a.'>,\n",
              "  <tf.Tensor: shape=(), dtype=string, numpy=b'-16*a**3*v - 336*a**3'>)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2SZRlVkVHkA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode(question,answer):\n",
        "  #Adds start token (tokenizer.vocab_size) and end token (tokenizer.vocab_size + 1) to (question,answer)\n",
        "\n",
        "  question = [tokenizer.vocab_size] + tokenizer.encode(question.numpy()) + [tokenizer.vocab_size + 1] \n",
        "  answer = [tokenizer.vocab_size] + tokenizer.encode(answer.numpy()) + [tokenizer.vocab_size + 1] \n",
        "\n",
        "  return question,answer\n",
        "\n",
        "def tf_encode(question, answer):\n",
        "  #We have to wrap encode in a tf.py_function() since the dataset elements do not have \n",
        "  question, answer = tf.py_function(encode, [question, answer], [tf.int64, tf.int64])\n",
        "  question.set_shape([None])\n",
        "  answer.set_shape([None])\n",
        "\n",
        "  return question, answer\n",
        "\n",
        "def tf_interleave_encode(question, answer):\n",
        "  #We have to wrap encode in a tf.py_function() since the dataset elements do not have \n",
        "  question, answer = tf.py_function(encode, [question, answer], [tf.int64, tf.int64])\n",
        "  question.set_shape([None])\n",
        "  answer.set_shape([None])\n",
        "\n",
        "  return tf.data.Dataset.from_tensors((question, answer))"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JduaOCVjd3_u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "5b23ea00-770c-477c-91d8-14c7001e83e1"
      },
      "source": [
        "train_dataset = train.take(10).map(tf_encode)\n",
        "list(train_dataset.take(1))"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(<tf.Tensor: shape=(29,), dtype=int64, numpy=\n",
              "  array([4086,   12,    4,   16,    5,   11,    7, 3882, 3872, 3927,    1,\n",
              "         3882, 3872, 3948,    3,  122, 3872, 3927,    1, 3882,    3, 3948,\n",
              "            3,  918, 3862,    6, 3927, 3876, 4087])>,\n",
              "  <tf.Tensor: shape=(16,), dtype=int64, numpy=\n",
              "  array([4086, 3875,   30, 3872, 3927,    1, 3881, 3872, 3948,    3,  554,\n",
              "         3872, 3927,    1, 3881, 4087])>)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcquKjDcaIqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_length_question = 35\n",
        "max_length_answer = 25\n",
        "def filter_max_length(x, y, max_length_question = max_length_question, max_length_answer = max_length_answer):\n",
        "  return tf.logical_and(tf.size(x) <= max_length_question,\n",
        "                        tf.size(y) <= max_length_answer)"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tT_0GTEufgY3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(dataset,batch_size, pad_len_question = max_length_question, pad_length_answer = max_length_answer):\n",
        "  dataset = dataset.cache()\n",
        "  #dataset = dataset.map(tf_encode)\n",
        "  dataset = dataset.interleave(tf_interleave_encode, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
        "  dataset = dataset.filter(filter_max_length)\n",
        "  dataset = dataset.shuffle(10000)\n",
        "  #dataset = dataset.padded_batch(batch_size)\n",
        "  pad = tf.cast(0,tf.int64)\n",
        "  dataset = dataset.padded_batch(batch_size, drop_remainder = True, padded_shapes = ([pad_len_question],[pad_length_answer]), padding_values = pad)\n",
        "  dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  return dataset"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7K3lE8cSIHe4",
        "colab_type": "text"
      },
      "source": [
        "# Positional Encoding and Masks\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spnqARtqKcY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def positional_encoding(pos, d_model):\n",
        "\n",
        "  pos_enc = np.zeros((1, pos , d_model))\n",
        "\n",
        "  for p in range(pos):\n",
        "    for i in range(d_model//2):\n",
        "      angles = p / np.power(10000, (2 * i) / np.float32(d_model))\n",
        "      pos_enc[:,p,2*i] = np.sin(angles)\n",
        "      pos_enc[:,p,2*i+1] = np.cos(angles)\n",
        "    if d_model % 2 == 1:\n",
        "      # if d_model is odd loop doesn't hit last even index\n",
        "      angles = p / np.power(10000, (2 * d_model) / np.float32(d_model))\n",
        "      pos_enc[:,p,d_model-1] = np.sin(angles)\n",
        "  return tf.cast(pos_enc, tf.float32)"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WepVhrhFT_hA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def padding_mask(seq):\n",
        "  #0's where the sequence is padded, 1 where it is not\n",
        "\n",
        "  mask = 1-tf.cast(tf.math.equal(seq,0),tf.float32)\n",
        "  return mask[:,:,tf.newaxis,tf.newaxis] #(batch, seq_len, 1, 1)"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_Bbh_hgIMaM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_mask(seq):\n",
        "  seq_len = tf.shape(seq)[1]\n",
        "  \"\"\"Returns a combined look_ahead_mask (lower triangular 1s)\n",
        "    and padding mask\"\"\"\n",
        "  look_ahead_mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0) \n",
        "  look_ahead_mask = look_ahead_mask [tf.newaxis,:,:,tf.newaxis]\n",
        "\n",
        "  padded_mask = padding_mask(seq)\n",
        "\n",
        "  return padded_mask*look_ahead_mask  #(batch, seq_len, seq_len, 1)"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD3pDp5shvpU",
        "colab_type": "text"
      },
      "source": [
        "# The Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mG-F06GoKcI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e0156c23-a51e-457b-f13e-542976e6abf8"
      },
      "source": [
        "#We will use tf.einsum to save ourselfs 3 tf.transpose operations during the calculation of the attention\n",
        "#This has the additional advantange of facilitating implementation of differente types of attention kernel.\n",
        "\n",
        "q = tf.random.uniform((5000,50,8, 100))\n",
        "k = tf.random.uniform((5000,45,8, 100))\n",
        "\n",
        "\n",
        "tstart = time.time()\n",
        "qt = tf.transpose(q, perm=[0, 2, 1, 3])\n",
        "kt = tf.transpose(k, perm=[0, 2, 1, 3])\n",
        "qk = tf.matmul(qt, kt, transpose_b=True)\n",
        "qk = tf.transpose(qk, perm=[0, 2, 1, 3])\n",
        "tend = time.time()\n",
        "\n",
        "print(\"With matmul :\", tend-tstart)\n",
        "\n",
        "tstart = time.time()\n",
        "qkeinsum = tf.einsum(\"mlhd,mjhd->mljh\",q, k)\n",
        "tend = time.time()\n",
        "print(\"With einsum :\", tend-tstart)\n",
        "\n"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With matmul : 0.0023474693298339844\n",
            "With einsum : 0.0009665489196777344\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ftz-STF_NN-D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7b9435fb-700a-4a34-90c6-e9ad26b24862"
      },
      "source": [
        "#the encoder qk has shape (batch, seq_len_question, seq_len_question, num_heads) enc_padding_mask goes here\n",
        "#the 1st decoder qk has shape (batch, seq_len_answer, seq_len_answer, num_heads) dec_forward mask goes here\n",
        "#the 2nd decoder qk has shape (batch, seq_len_answer, seq_len_question, num_heads) dec_padding_mask goes here\n",
        "\n",
        "seq_question = tf.random.uniform((5000,45))\n",
        "seq_answer = tf.random.uniform((5000,50))\n",
        "pad_mask = padding_mask(seq_answer)\n",
        "print(tf.shape(pad_mask))\n",
        "for_mask = forward_mask(seq_answer)\n",
        "print(tf.shape(for_mask))\n",
        "print(tf.shape(qkeinsum*pad_mask))"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([5000   50    1    1], shape=(4,), dtype=int32)\n",
            "tf.Tensor([5000   50   50    1], shape=(4,), dtype=int32)\n",
            "tf.Tensor([5000   50   45    8], shape=(4,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOJjdEEghvGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self,d_model,num_heads):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    assert d_model%self.num_heads == 0\n",
        "\n",
        "    self.depth=d_model//self.num_heads\n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def split_heads(self,x, batch_size):\n",
        "\n",
        "    \"\"\"Split the last dimension into (num_heads,depth)\n",
        "\n",
        "    Arguments:\n",
        "    x -- A tokenized sequence (batch_size, seq_len, d_model)\n",
        "    \n",
        "    Returns:\n",
        "    A tokenized sequence with dimensions (batch_size, seq_len, num_heads, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "\n",
        "    return x \n",
        "\n",
        "  def call(self,q,k,v,mask=None):\n",
        "\n",
        "    batch_size = tf.shape(q)[0]\n",
        "    q = self.wq(q) #(batch_size,len_q, dim_q) \n",
        "    k = self.wk(k) #(batch_size,len_v, dim_q) \n",
        "    v = self.wv(v) #(batch_size,len_v, dim_v) \n",
        "    \n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, len_q, num_heads, depth_q) (m,l,h,d)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, len_v, num_heads, depth_q) (m,j,h,d)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, len_v, num_heads, depth_v) (m,j,h,e)\n",
        "\n",
        "    qk = tf.einsum(\"mlhd,mjhd->mljh\",q,k) #(batch_size, len_q, len_v, num_heads) (m,l,j,h)\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32) \n",
        "    qk = qk/tf.math.sqrt(dk)\n",
        "\n",
        "    if mask is not None: \n",
        "       qk = qk * mask # We are using a multiplicative mask\n",
        "\n",
        "    qk = tf.nn.softmax(qk, axis = -1) #(batch_size,len_q,len_v, num_heads) (m,l,j,h)\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    qk = qk/tf.math.sqrt(dk)\n",
        "\n",
        "    output = tf.einsum(\"mljh, mjhe -> mlhe\",qk,v) #(batch_size,len_q, heads, depth_v)\n",
        "    output = tf.reshape(output,(batch_size, -1, self.num_heads*self.depth)) #(batch_size,len_q, d_model)\n",
        "\n",
        "    return self.dense(output)\n"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nJ22J-8woib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  \n",
        "  \"\"\"The EncoderLayer consisters of one MultiHeadAttention layer connected to a FeedForward layer,\n",
        "  each of these 2 layers have a residual connection.\"\"\"\n",
        "\n",
        "  def __init__(self, num_heads, d_model, dense_dim, dropout = 0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.attention = MultiHeadAttention(d_model,num_heads)\n",
        "    self.dense = tf.keras.Sequential([tf.keras.layers.Dense(dense_dim,activation='relu'),\n",
        "                                         tf.keras.layers.Dense(d_model)])\n",
        "    \n",
        "    self.norm1 = tf.keras.layers.LayerNormalization()\n",
        "    self.norm2 = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(dropout)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    out_attention = self.attention(x, x, x, mask) #(batch_size,seq_len,d_model)\n",
        "    out_attention = self.dropout1(out_attention, training=training)\n",
        "    out1 = self.norm1(x + out_attention) #residual connection (batch_size,seq_len,d_model)\n",
        "\n",
        "    out_dense = self.dense(out1) #(batch_size,seq_len,d_model)\n",
        "    out2 = self.norm2(out1 + out_dense) #residual conenction (batch_size,seq_len,d_model)\n",
        "    return out2\n",
        "\n",
        "    \n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, num_layers, num_heads, d_model, dense_dim,\n",
        "               vocab_size, max_encoding_position, dropout  = 0.1):\n",
        "    super().__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers \n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
        "    self.positional_encoding = positional_encoding(max_encoding_position, d_model)\n",
        "    self.encoding_layers = [EncoderLayer(num_heads, d_model, dense_dim, dropout) for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "  def call(self, x, training, mask = None):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    x = self.embedding(x) #(batch_size,input_len,d_model)\n",
        "    x = x * tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x = x + self.positional_encoding[:, :seq_len, :] \n",
        "    x = self.dropout(x, training = training)    \n",
        "    for i in range(self.num_layers):\n",
        "      x = self.encoding_layers[i](x, training, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    return x "
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yoq8-hnE6dEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "\n",
        "\n",
        "  def __init__(self, num_heads, d_model, dense_dim, dropout = 0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.attention1 = MultiHeadAttention(d_model,num_heads)\n",
        "    self.attention2 = MultiHeadAttention(d_model,num_heads)\n",
        "\n",
        "    self.dense = tf.keras.Sequential([tf.keras.layers.Dense(dense_dim,activation='relu'),\n",
        "                                        tf.keras.layers.Dense(d_model)])\n",
        "\n",
        "    \n",
        "    self.norm1 = tf.keras.layers.LayerNormalization()\n",
        "    self.norm2 = tf.keras.layers.LayerNormalization()\n",
        "    self.norm3 = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(dropout)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(dropout)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "  def call(self, encoder_out, x, training, forward_mask, padding_mask):\n",
        "\n",
        "    #We will not use forward masking since it makes little sense in this context\n",
        "\n",
        "    out_attention1 = self.attention1(x, x, x, forward_mask) #(batch_size, seq_len_answer, d_model) -> The return seq_len is the same as that of the first argument of the call.\n",
        "    out_attention1 = self.dropout1(out_attention1, training = training)\n",
        "    out1 = self.norm1(x + out_attention1) #residual connection (batch_size, seq_len_answer, d_model)\n",
        "\n",
        "    out_attention2 = self.attention2(out1, encoder_out, encoder_out, padding_mask) #(batch_size, seq_len_answer, d_model)\n",
        "    out_attention2 = self.dropout2(out_attention2, training = training)\n",
        "    out2 = self.norm2(out1 + out_attention2)\n",
        "\n",
        "    out_dense = self.dense(out2)\n",
        "    out_dense = self.dropout3(out_dense + out2)\n",
        "\n",
        "    return out_dense\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "\n",
        "\n",
        "  def __init__(self, num_layers, num_heads, d_model, dense_dim,\n",
        "               vocab_size, max_encoding_position, dropout  = 0.1):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers \n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
        "    self.positional_encoding = positional_encoding(max_encoding_position, d_model)\n",
        "    self.decoder_layers = [DecoderLayer(num_heads, d_model, dense_dim, dropout) for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "\n",
        "  def call(self, encoder_out, x, training, forward_mask = None, padding_mask = None):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    x = self.embedding(x) #(batch_size,input_len,d_model)\n",
        "    x = x * tf.math.sqrt(tf.cast(self.d_model, tf.float32)) \n",
        "    x = x + self.positional_encoding[:, :seq_len, :] \n",
        "    x = self.dropout(x, training = training)    \n",
        "    for i in range(self.num_layers):\n",
        "      x = self.decoder_layers[i](encoder_out, x, training, forward_mask, padding_mask)  # (batch_size, input_seq_len, d_model)\n",
        "    return x"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecrEgr2AF5eV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "\n",
        "  \n",
        "    def __init__(self, num_layers, num_heads, d_model,  dense_dim, vocab_size,\n",
        "                 input_max_position, target_max_position, rate=0.1):\n",
        "      super().__init__()\n",
        "\n",
        "      self.encoder = Encoder(num_layers, num_heads, d_model, dense_dim,\n",
        "               vocab_size, max_encoding_position = input_max_position, dropout  = 0.1)\n",
        "      \n",
        "      self.decoder = Decoder(num_layers, num_heads, d_model, dense_dim,\n",
        "               vocab_size, max_encoding_position = target_max_position, dropout  = 0.1)\n",
        "      \n",
        "      self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, input, target, training = False, enc_mask = None , dec_forward_mask= None, dec_padding_mask = None):\n",
        "\n",
        "      out_encoder = self.encoder(input, training = training, mask = enc_mask)\n",
        "\n",
        "      out_decoder = self.decoder(out_encoder, target, training = training, forward_mask = dec_forward_mask, padding_mask = dec_padding_mask)\n",
        "\n",
        "      out = self.dense(out_decoder)\n",
        "\n",
        "      return out"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRoizp-KmQge",
        "colab_type": "text"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oGk0yQKdPPB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_layers = 5\n",
        "d_model = 256\n",
        "dense_dim = 512\n",
        "num_heads = 8\n",
        "\n",
        "vocab_size = tokenizer.vocab_size + 2\n",
        "\n",
        "dropout_rate = 0.1\n",
        "transformer = Transformer( num_layers = num_layers, num_heads = num_heads, d_model = d_model,  dense_dim = dense_dim, vocab_size = vocab_size,\n",
        "                 input_max_position = max_length_question, target_max_position = max_length_answer, rate=0.1)"
      ],
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeNWYTa018fQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "    \n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "    \n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "    \n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAq8RIZKfLbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def masked_loss_fn(answer, prediction):\n",
        "  mask = tf.math.logical_not(tf.math.equal(answer,0)) #0 at zeroes, 1 at non-zeroes since seq is padded\n",
        "  mask = tf.cast(mask, tf.int32)\n",
        "  loss_value = loss_fn(answer,prediction,sample_weight=mask) #set the zeros to zero weight, other values have weight of 1. \n",
        "\n",
        "  return loss_value\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neGZo1pI31rO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b79602bf-3c65-4f4a-eb52-783fc45699ad"
      },
      "source": [
        "EPOCHS = 200\n",
        "signature = [tf.TensorSpec(shape=(None, max_length_question), dtype=tf.int64), tf.TensorSpec(shape=(None, max_length_answer), dtype=tf.int64),] #quite a bit faster if we specify the signature\n",
        "\n",
        "@tf.function(input_signature=signature)\n",
        "def train_step(question, answer):\n",
        "  answer_in = answer[:, :-1]\n",
        "  answer_tar = answer[:, 1:]\n",
        "  \n",
        "  enc_padding_mask = padding_mask(question)\n",
        "  dec_padding_mask = padding_mask(answer_in)\n",
        "  dec_forward_mask = forward_mask(answer_in)\n",
        "  \n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = transformer(question, answer_in, training = True, enc_mask = enc_padding_mask , dec_forward_mask = dec_forward_mask, dec_padding_mask = dec_padding_mask)\n",
        "    loss = masked_loss_fn(answer_tar, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "  \n",
        "  train_loss(loss)\n",
        "  train_accuracy(answer_tar, predictions)\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  \n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  \n",
        "  for (batch, (question, answer)) in enumerate(train_dataset):\n",
        "    train_step(question, answer)\n",
        "\n",
        "  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
        "                                                train_loss.result(), \n",
        "                                                train_accuracy.result()))\n",
        "\n",
        "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss 2.1263 Accuracy 0.0423\n",
            "Time taken for 1 epoch: 15.727577447891235 secs\n",
            "\n",
            "Epoch 2 Loss 1.5050 Accuracy 0.0490\n",
            "Time taken for 1 epoch: 1.4465632438659668 secs\n",
            "\n",
            "Epoch 3 Loss 1.4552 Accuracy 0.0529\n",
            "Time taken for 1 epoch: 1.4330511093139648 secs\n",
            "\n",
            "Epoch 4 Loss 1.4476 Accuracy 0.0519\n",
            "Time taken for 1 epoch: 1.4335923194885254 secs\n",
            "\n",
            "Epoch 5 Loss 1.4130 Accuracy 0.0574\n",
            "Time taken for 1 epoch: 1.4854414463043213 secs\n",
            "\n",
            "Epoch 6 Loss 1.3969 Accuracy 0.0679\n",
            "Time taken for 1 epoch: 1.4531705379486084 secs\n",
            "\n",
            "Epoch 7 Loss 1.3563 Accuracy 0.0869\n",
            "Time taken for 1 epoch: 1.4612524509429932 secs\n",
            "\n",
            "Epoch 8 Loss 1.3182 Accuracy 0.0846\n",
            "Time taken for 1 epoch: 1.4470632076263428 secs\n",
            "\n",
            "Epoch 9 Loss 1.2751 Accuracy 0.0942\n",
            "Time taken for 1 epoch: 1.4446589946746826 secs\n",
            "\n",
            "Epoch 10 Loss 1.2541 Accuracy 0.0939\n",
            "Time taken for 1 epoch: 1.4217946529388428 secs\n",
            "\n",
            "Epoch 11 Loss 1.2130 Accuracy 0.0987\n",
            "Time taken for 1 epoch: 1.4182827472686768 secs\n",
            "\n",
            "Epoch 12 Loss 1.1835 Accuracy 0.1026\n",
            "Time taken for 1 epoch: 1.4518060684204102 secs\n",
            "\n",
            "Epoch 13 Loss 1.1383 Accuracy 0.1054\n",
            "Time taken for 1 epoch: 1.4500107765197754 secs\n",
            "\n",
            "Epoch 14 Loss 1.1243 Accuracy 0.1042\n",
            "Time taken for 1 epoch: 1.4392170906066895 secs\n",
            "\n",
            "Epoch 15 Loss 1.1348 Accuracy 0.1016\n",
            "Time taken for 1 epoch: 1.4282374382019043 secs\n",
            "\n",
            "Epoch 16 Loss 1.0935 Accuracy 0.1074\n",
            "Time taken for 1 epoch: 1.408141851425171 secs\n",
            "\n",
            "Epoch 17 Loss 1.0660 Accuracy 0.1112\n",
            "Time taken for 1 epoch: 1.4594781398773193 secs\n",
            "\n",
            "Epoch 18 Loss 1.0443 Accuracy 0.1115\n",
            "Time taken for 1 epoch: 1.4082269668579102 secs\n",
            "\n",
            "Epoch 19 Loss 1.0428 Accuracy 0.1087\n",
            "Time taken for 1 epoch: 1.4198670387268066 secs\n",
            "\n",
            "Epoch 20 Loss 1.0357 Accuracy 0.1122\n",
            "Time taken for 1 epoch: 1.3839635848999023 secs\n",
            "\n",
            "Epoch 21 Loss 1.0314 Accuracy 0.1160\n",
            "Time taken for 1 epoch: 1.411731481552124 secs\n",
            "\n",
            "Epoch 22 Loss 1.0054 Accuracy 0.1173\n",
            "Time taken for 1 epoch: 1.4291493892669678 secs\n",
            "\n",
            "Epoch 23 Loss 0.9859 Accuracy 0.1173\n",
            "Time taken for 1 epoch: 1.4039909839630127 secs\n",
            "\n",
            "Epoch 24 Loss 0.9836 Accuracy 0.1157\n",
            "Time taken for 1 epoch: 1.4076552391052246 secs\n",
            "\n",
            "Epoch 25 Loss 0.9729 Accuracy 0.1154\n",
            "Time taken for 1 epoch: 1.424968957901001 secs\n",
            "\n",
            "Epoch 26 Loss 0.9551 Accuracy 0.1179\n",
            "Time taken for 1 epoch: 1.448824167251587 secs\n",
            "\n",
            "Epoch 27 Loss 0.9427 Accuracy 0.1192\n",
            "Time taken for 1 epoch: 1.424051284790039 secs\n",
            "\n",
            "Epoch 28 Loss 0.9338 Accuracy 0.1263\n",
            "Time taken for 1 epoch: 1.4256718158721924 secs\n",
            "\n",
            "Epoch 29 Loss 0.9266 Accuracy 0.1189\n",
            "Time taken for 1 epoch: 1.439342975616455 secs\n",
            "\n",
            "Epoch 30 Loss 0.9182 Accuracy 0.1234\n",
            "Time taken for 1 epoch: 1.453228235244751 secs\n",
            "\n",
            "Epoch 31 Loss 0.9061 Accuracy 0.1202\n",
            "Time taken for 1 epoch: 1.4360008239746094 secs\n",
            "\n",
            "Epoch 32 Loss 0.9082 Accuracy 0.1272\n",
            "Time taken for 1 epoch: 1.4468276500701904 secs\n",
            "\n",
            "Epoch 33 Loss 0.9034 Accuracy 0.1212\n",
            "Time taken for 1 epoch: 1.4459881782531738 secs\n",
            "\n",
            "Epoch 34 Loss 0.9011 Accuracy 0.1224\n",
            "Time taken for 1 epoch: 1.415889024734497 secs\n",
            "\n",
            "Epoch 35 Loss 0.8897 Accuracy 0.1288\n",
            "Time taken for 1 epoch: 1.439532995223999 secs\n",
            "\n",
            "Epoch 36 Loss 0.8799 Accuracy 0.1244\n",
            "Time taken for 1 epoch: 1.457369327545166 secs\n",
            "\n",
            "Epoch 37 Loss 0.8884 Accuracy 0.1285\n",
            "Time taken for 1 epoch: 1.404099941253662 secs\n",
            "\n",
            "Epoch 38 Loss 0.8687 Accuracy 0.1266\n",
            "Time taken for 1 epoch: 1.4260327816009521 secs\n",
            "\n",
            "Epoch 39 Loss 0.8566 Accuracy 0.1279\n",
            "Time taken for 1 epoch: 1.4354541301727295 secs\n",
            "\n",
            "Epoch 40 Loss 0.8613 Accuracy 0.1244\n",
            "Time taken for 1 epoch: 1.4944875240325928 secs\n",
            "\n",
            "Epoch 41 Loss 0.8585 Accuracy 0.1279\n",
            "Time taken for 1 epoch: 1.41621732711792 secs\n",
            "\n",
            "Epoch 42 Loss 0.8459 Accuracy 0.1317\n",
            "Time taken for 1 epoch: 1.4438045024871826 secs\n",
            "\n",
            "Epoch 43 Loss 0.8405 Accuracy 0.1269\n",
            "Time taken for 1 epoch: 1.4437074661254883 secs\n",
            "\n",
            "Epoch 44 Loss 0.8465 Accuracy 0.1324\n",
            "Time taken for 1 epoch: 1.4439537525177002 secs\n",
            "\n",
            "Epoch 45 Loss 0.8368 Accuracy 0.1279\n",
            "Time taken for 1 epoch: 1.4209160804748535 secs\n",
            "\n",
            "Epoch 46 Loss 0.8185 Accuracy 0.1295\n",
            "Time taken for 1 epoch: 1.3962736129760742 secs\n",
            "\n",
            "Epoch 47 Loss 0.8344 Accuracy 0.1295\n",
            "Time taken for 1 epoch: 1.435450792312622 secs\n",
            "\n",
            "Epoch 48 Loss 0.8128 Accuracy 0.1288\n",
            "Time taken for 1 epoch: 1.418057918548584 secs\n",
            "\n",
            "Epoch 49 Loss 0.8019 Accuracy 0.1321\n",
            "Time taken for 1 epoch: 1.4108898639678955 secs\n",
            "\n",
            "Epoch 50 Loss 0.8056 Accuracy 0.1321\n",
            "Time taken for 1 epoch: 1.4201102256774902 secs\n",
            "\n",
            "Epoch 51 Loss 0.8061 Accuracy 0.1272\n",
            "Time taken for 1 epoch: 1.4400839805603027 secs\n",
            "\n",
            "Epoch 52 Loss 0.8160 Accuracy 0.1301\n",
            "Time taken for 1 epoch: 1.4107775688171387 secs\n",
            "\n",
            "Epoch 53 Loss 0.7986 Accuracy 0.1282\n",
            "Time taken for 1 epoch: 1.4290356636047363 secs\n",
            "\n",
            "Epoch 54 Loss 0.7856 Accuracy 0.1346\n",
            "Time taken for 1 epoch: 1.429490327835083 secs\n",
            "\n",
            "Epoch 55 Loss 0.7917 Accuracy 0.1337\n",
            "Time taken for 1 epoch: 1.424663782119751 secs\n",
            "\n",
            "Epoch 56 Loss 0.7868 Accuracy 0.1285\n",
            "Time taken for 1 epoch: 1.4125304222106934 secs\n",
            "\n",
            "Epoch 57 Loss 0.7648 Accuracy 0.1359\n",
            "Time taken for 1 epoch: 1.4063348770141602 secs\n",
            "\n",
            "Epoch 58 Loss 0.7949 Accuracy 0.1311\n",
            "Time taken for 1 epoch: 1.4609954357147217 secs\n",
            "\n",
            "Epoch 59 Loss 0.7882 Accuracy 0.1308\n",
            "Time taken for 1 epoch: 1.4258630275726318 secs\n",
            "\n",
            "Epoch 60 Loss 0.7532 Accuracy 0.1356\n",
            "Time taken for 1 epoch: 1.4354958534240723 secs\n",
            "\n",
            "Epoch 61 Loss 0.7776 Accuracy 0.1311\n",
            "Time taken for 1 epoch: 1.4492475986480713 secs\n",
            "\n",
            "Epoch 62 Loss 0.7724 Accuracy 0.1253\n",
            "Time taken for 1 epoch: 1.415259599685669 secs\n",
            "\n",
            "Epoch 63 Loss 0.7641 Accuracy 0.1327\n",
            "Time taken for 1 epoch: 1.454261064529419 secs\n",
            "\n",
            "Epoch 64 Loss 0.7571 Accuracy 0.1362\n",
            "Time taken for 1 epoch: 1.4118051528930664 secs\n",
            "\n",
            "Epoch 65 Loss 0.7419 Accuracy 0.1356\n",
            "Time taken for 1 epoch: 1.4594171047210693 secs\n",
            "\n",
            "Epoch 66 Loss 0.7357 Accuracy 0.1356\n",
            "Time taken for 1 epoch: 1.417816400527954 secs\n",
            "\n",
            "Epoch 67 Loss 0.7435 Accuracy 0.1346\n",
            "Time taken for 1 epoch: 1.430910348892212 secs\n",
            "\n",
            "Epoch 68 Loss 0.7369 Accuracy 0.1349\n",
            "Time taken for 1 epoch: 1.4463543891906738 secs\n",
            "\n",
            "Epoch 69 Loss 1.6614 Accuracy 0.0952\n",
            "Time taken for 1 epoch: 1.4448740482330322 secs\n",
            "\n",
            "Epoch 70 Loss 0.9207 Accuracy 0.1128\n",
            "Time taken for 1 epoch: 1.4629299640655518 secs\n",
            "\n",
            "Epoch 71 Loss 0.8285 Accuracy 0.1247\n",
            "Time taken for 1 epoch: 1.3991117477416992 secs\n",
            "\n",
            "Epoch 72 Loss 0.7786 Accuracy 0.1321\n",
            "Time taken for 1 epoch: 1.3991003036499023 secs\n",
            "\n",
            "Epoch 73 Loss 0.7387 Accuracy 0.1394\n",
            "Time taken for 1 epoch: 1.446406602859497 secs\n",
            "\n",
            "Epoch 74 Loss 0.7472 Accuracy 0.1375\n",
            "Time taken for 1 epoch: 1.4173634052276611 secs\n",
            "\n",
            "Epoch 75 Loss 0.7244 Accuracy 0.1397\n",
            "Time taken for 1 epoch: 1.4431564807891846 secs\n",
            "\n",
            "Epoch 76 Loss 0.7345 Accuracy 0.1420\n",
            "Time taken for 1 epoch: 1.4053473472595215 secs\n",
            "\n",
            "Epoch 77 Loss 0.7145 Accuracy 0.1391\n",
            "Time taken for 1 epoch: 1.4054994583129883 secs\n",
            "\n",
            "Epoch 78 Loss 0.7176 Accuracy 0.1413\n",
            "Time taken for 1 epoch: 1.4493517875671387 secs\n",
            "\n",
            "Epoch 79 Loss 0.7044 Accuracy 0.1439\n",
            "Time taken for 1 epoch: 1.4468553066253662 secs\n",
            "\n",
            "Epoch 80 Loss 0.7100 Accuracy 0.1442\n",
            "Time taken for 1 epoch: 1.4232611656188965 secs\n",
            "\n",
            "Epoch 81 Loss 0.7181 Accuracy 0.1381\n",
            "Time taken for 1 epoch: 1.4449472427368164 secs\n",
            "\n",
            "Epoch 82 Loss 0.7159 Accuracy 0.1420\n",
            "Time taken for 1 epoch: 1.4614896774291992 secs\n",
            "\n",
            "Epoch 83 Loss 0.7022 Accuracy 0.1413\n",
            "Time taken for 1 epoch: 1.4033880233764648 secs\n",
            "\n",
            "Epoch 84 Loss 0.7156 Accuracy 0.1365\n",
            "Time taken for 1 epoch: 1.4089572429656982 secs\n",
            "\n",
            "Epoch 85 Loss 0.7088 Accuracy 0.1449\n",
            "Time taken for 1 epoch: 1.3979082107543945 secs\n",
            "\n",
            "Epoch 86 Loss 0.7007 Accuracy 0.1436\n",
            "Time taken for 1 epoch: 1.5110747814178467 secs\n",
            "\n",
            "Epoch 87 Loss 0.7062 Accuracy 0.1455\n",
            "Time taken for 1 epoch: 1.5024657249450684 secs\n",
            "\n",
            "Epoch 88 Loss 0.6813 Accuracy 0.1439\n",
            "Time taken for 1 epoch: 1.5297927856445312 secs\n",
            "\n",
            "Epoch 89 Loss 0.6853 Accuracy 0.1468\n",
            "Time taken for 1 epoch: 1.532480239868164 secs\n",
            "\n",
            "Epoch 90 Loss 0.6879 Accuracy 0.1433\n",
            "Time taken for 1 epoch: 1.5278270244598389 secs\n",
            "\n",
            "Epoch 91 Loss 0.6838 Accuracy 0.1474\n",
            "Time taken for 1 epoch: 1.52754545211792 secs\n",
            "\n",
            "Epoch 92 Loss 0.6895 Accuracy 0.1417\n",
            "Time taken for 1 epoch: 1.4742887020111084 secs\n",
            "\n",
            "Epoch 93 Loss 0.6760 Accuracy 0.1410\n",
            "Time taken for 1 epoch: 1.3937957286834717 secs\n",
            "\n",
            "Epoch 94 Loss 0.6798 Accuracy 0.1449\n",
            "Time taken for 1 epoch: 1.3809113502502441 secs\n",
            "\n",
            "Epoch 95 Loss 0.6486 Accuracy 0.1490\n",
            "Time taken for 1 epoch: 1.419234037399292 secs\n",
            "\n",
            "Epoch 96 Loss 0.6828 Accuracy 0.1417\n",
            "Time taken for 1 epoch: 1.4149479866027832 secs\n",
            "\n",
            "Epoch 97 Loss 0.6771 Accuracy 0.1426\n",
            "Time taken for 1 epoch: 1.4047694206237793 secs\n",
            "\n",
            "Epoch 98 Loss 0.6614 Accuracy 0.1446\n",
            "Time taken for 1 epoch: 1.413809061050415 secs\n",
            "\n",
            "Epoch 99 Loss 0.6574 Accuracy 0.1436\n",
            "Time taken for 1 epoch: 1.407534122467041 secs\n",
            "\n",
            "Epoch 100 Loss 0.6749 Accuracy 0.1494\n",
            "Time taken for 1 epoch: 1.4272651672363281 secs\n",
            "\n",
            "Epoch 101 Loss 0.6695 Accuracy 0.1413\n",
            "Time taken for 1 epoch: 1.4043047428131104 secs\n",
            "\n",
            "Epoch 102 Loss 0.6710 Accuracy 0.1478\n",
            "Time taken for 1 epoch: 1.4115283489227295 secs\n",
            "\n",
            "Epoch 103 Loss 0.6659 Accuracy 0.1474\n",
            "Time taken for 1 epoch: 1.3957202434539795 secs\n",
            "\n",
            "Epoch 104 Loss 0.6690 Accuracy 0.1413\n",
            "Time taken for 1 epoch: 1.387866735458374 secs\n",
            "\n",
            "Epoch 105 Loss 0.6440 Accuracy 0.1436\n",
            "Time taken for 1 epoch: 1.4134023189544678 secs\n",
            "\n",
            "Epoch 106 Loss 0.6423 Accuracy 0.1522\n",
            "Time taken for 1 epoch: 1.4059863090515137 secs\n",
            "\n",
            "Epoch 107 Loss 0.6447 Accuracy 0.1510\n",
            "Time taken for 1 epoch: 1.393902063369751 secs\n",
            "\n",
            "Epoch 108 Loss 0.6527 Accuracy 0.1458\n",
            "Time taken for 1 epoch: 1.4009840488433838 secs\n",
            "\n",
            "Epoch 109 Loss 0.6591 Accuracy 0.1433\n",
            "Time taken for 1 epoch: 1.402059555053711 secs\n",
            "\n",
            "Epoch 110 Loss 0.7172 Accuracy 0.1388\n",
            "Time taken for 1 epoch: 1.4311902523040771 secs\n",
            "\n",
            "Epoch 111 Loss 0.6729 Accuracy 0.1413\n",
            "Time taken for 1 epoch: 1.417586088180542 secs\n",
            "\n",
            "Epoch 112 Loss 0.6633 Accuracy 0.1413\n",
            "Time taken for 1 epoch: 1.3845899105072021 secs\n",
            "\n",
            "Epoch 113 Loss 0.6410 Accuracy 0.1446\n",
            "Time taken for 1 epoch: 1.3765313625335693 secs\n",
            "\n",
            "Epoch 114 Loss 0.6881 Accuracy 0.1497\n",
            "Time taken for 1 epoch: 1.4011318683624268 secs\n",
            "\n",
            "Epoch 115 Loss 0.6419 Accuracy 0.1490\n",
            "Time taken for 1 epoch: 1.380737543106079 secs\n",
            "\n",
            "Epoch 116 Loss 0.6332 Accuracy 0.1535\n",
            "Time taken for 1 epoch: 1.3886380195617676 secs\n",
            "\n",
            "Epoch 117 Loss 0.6199 Accuracy 0.1545\n",
            "Time taken for 1 epoch: 1.4101872444152832 secs\n",
            "\n",
            "Epoch 118 Loss 0.5948 Accuracy 0.1558\n",
            "Time taken for 1 epoch: 1.3978257179260254 secs\n",
            "\n",
            "Epoch 119 Loss 0.6153 Accuracy 0.1532\n",
            "Time taken for 1 epoch: 1.4144513607025146 secs\n",
            "\n",
            "Epoch 120 Loss 0.6214 Accuracy 0.1567\n",
            "Time taken for 1 epoch: 1.3964264392852783 secs\n",
            "\n",
            "Epoch 121 Loss 0.6109 Accuracy 0.1529\n",
            "Time taken for 1 epoch: 1.4004240036010742 secs\n",
            "\n",
            "Epoch 122 Loss 0.6168 Accuracy 0.1510\n",
            "Time taken for 1 epoch: 1.3955357074737549 secs\n",
            "\n",
            "Epoch 123 Loss 0.6006 Accuracy 0.1561\n",
            "Time taken for 1 epoch: 1.3768079280853271 secs\n",
            "\n",
            "Epoch 124 Loss 0.5876 Accuracy 0.1596\n",
            "Time taken for 1 epoch: 1.3973047733306885 secs\n",
            "\n",
            "Epoch 125 Loss 0.6099 Accuracy 0.1574\n",
            "Time taken for 1 epoch: 1.44842529296875 secs\n",
            "\n",
            "Epoch 126 Loss 0.6088 Accuracy 0.1513\n",
            "Time taken for 1 epoch: 1.3910953998565674 secs\n",
            "\n",
            "Epoch 127 Loss 0.6027 Accuracy 0.1526\n",
            "Time taken for 1 epoch: 1.4099974632263184 secs\n",
            "\n",
            "Epoch 128 Loss 0.5822 Accuracy 0.1612\n",
            "Time taken for 1 epoch: 1.381995677947998 secs\n",
            "\n",
            "Epoch 129 Loss 0.5946 Accuracy 0.1615\n",
            "Time taken for 1 epoch: 1.3725731372833252 secs\n",
            "\n",
            "Epoch 130 Loss 0.6080 Accuracy 0.1554\n",
            "Time taken for 1 epoch: 1.411346673965454 secs\n",
            "\n",
            "Epoch 131 Loss 0.5923 Accuracy 0.1548\n",
            "Time taken for 1 epoch: 1.3939261436462402 secs\n",
            "\n",
            "Epoch 132 Loss 0.5684 Accuracy 0.1657\n",
            "Time taken for 1 epoch: 1.3900244235992432 secs\n",
            "\n",
            "Epoch 133 Loss 0.5870 Accuracy 0.1567\n",
            "Time taken for 1 epoch: 1.4039669036865234 secs\n",
            "\n",
            "Epoch 134 Loss 0.5594 Accuracy 0.1737\n",
            "Time taken for 1 epoch: 1.396913766860962 secs\n",
            "\n",
            "Epoch 135 Loss 0.5764 Accuracy 0.1641\n",
            "Time taken for 1 epoch: 1.4119453430175781 secs\n",
            "\n",
            "Epoch 136 Loss 0.5677 Accuracy 0.1654\n",
            "Time taken for 1 epoch: 1.4110982418060303 secs\n",
            "\n",
            "Epoch 137 Loss 0.5576 Accuracy 0.1660\n",
            "Time taken for 1 epoch: 1.4062678813934326 secs\n",
            "\n",
            "Epoch 138 Loss 0.5687 Accuracy 0.1641\n",
            "Time taken for 1 epoch: 1.3976185321807861 secs\n",
            "\n",
            "Epoch 139 Loss 0.5841 Accuracy 0.1670\n",
            "Time taken for 1 epoch: 1.4140923023223877 secs\n",
            "\n",
            "Epoch 140 Loss 0.5811 Accuracy 0.1574\n",
            "Time taken for 1 epoch: 1.40484619140625 secs\n",
            "\n",
            "Epoch 141 Loss 0.5792 Accuracy 0.1583\n",
            "Time taken for 1 epoch: 1.4114346504211426 secs\n",
            "\n",
            "Epoch 142 Loss 0.5826 Accuracy 0.1571\n",
            "Time taken for 1 epoch: 1.3870887756347656 secs\n",
            "\n",
            "Epoch 143 Loss 0.5574 Accuracy 0.1657\n",
            "Time taken for 1 epoch: 1.3802354335784912 secs\n",
            "\n",
            "Epoch 144 Loss 0.5720 Accuracy 0.1631\n",
            "Time taken for 1 epoch: 1.3919150829315186 secs\n",
            "\n",
            "Epoch 145 Loss 0.5670 Accuracy 0.1651\n",
            "Time taken for 1 epoch: 1.3689055442810059 secs\n",
            "\n",
            "Epoch 146 Loss 0.5519 Accuracy 0.1670\n",
            "Time taken for 1 epoch: 1.3787147998809814 secs\n",
            "\n",
            "Epoch 147 Loss 0.5603 Accuracy 0.1654\n",
            "Time taken for 1 epoch: 1.442553997039795 secs\n",
            "\n",
            "Epoch 148 Loss 0.6243 Accuracy 0.1571\n",
            "Time taken for 1 epoch: 1.3880891799926758 secs\n",
            "\n",
            "Epoch 149 Loss 0.6024 Accuracy 0.1551\n",
            "Time taken for 1 epoch: 1.3903322219848633 secs\n",
            "\n",
            "Epoch 150 Loss 0.5955 Accuracy 0.1628\n",
            "Time taken for 1 epoch: 1.4156475067138672 secs\n",
            "\n",
            "Epoch 151 Loss 0.5455 Accuracy 0.1718\n",
            "Time taken for 1 epoch: 1.378113031387329 secs\n",
            "\n",
            "Epoch 152 Loss 0.5300 Accuracy 0.1747\n",
            "Time taken for 1 epoch: 1.3794891834259033 secs\n",
            "\n",
            "Epoch 153 Loss 0.5174 Accuracy 0.1795\n",
            "Time taken for 1 epoch: 1.379258632659912 secs\n",
            "\n",
            "Epoch 154 Loss 0.5151 Accuracy 0.1788\n",
            "Time taken for 1 epoch: 1.3630967140197754 secs\n",
            "\n",
            "Epoch 155 Loss 0.5114 Accuracy 0.1827\n",
            "Time taken for 1 epoch: 1.4355864524841309 secs\n",
            "\n",
            "Epoch 156 Loss 0.5047 Accuracy 0.1824\n",
            "Time taken for 1 epoch: 1.398085355758667 secs\n",
            "\n",
            "Epoch 157 Loss 0.5032 Accuracy 0.1885\n",
            "Time taken for 1 epoch: 1.4040813446044922 secs\n",
            "\n",
            "Epoch 158 Loss 0.4987 Accuracy 0.1776\n",
            "Time taken for 1 epoch: 1.4008653163909912 secs\n",
            "\n",
            "Epoch 159 Loss 0.5049 Accuracy 0.1843\n",
            "Time taken for 1 epoch: 1.4392757415771484 secs\n",
            "\n",
            "Epoch 160 Loss 0.5006 Accuracy 0.1753\n",
            "Time taken for 1 epoch: 1.5021896362304688 secs\n",
            "\n",
            "Epoch 161 Loss 0.4975 Accuracy 0.1849\n",
            "Time taken for 1 epoch: 1.478555679321289 secs\n",
            "\n",
            "Epoch 162 Loss 0.4958 Accuracy 0.1840\n",
            "Time taken for 1 epoch: 1.4945948123931885 secs\n",
            "\n",
            "Epoch 163 Loss 0.4947 Accuracy 0.1827\n",
            "Time taken for 1 epoch: 1.3828535079956055 secs\n",
            "\n",
            "Epoch 164 Loss 0.4705 Accuracy 0.1891\n",
            "Time taken for 1 epoch: 1.447939157485962 secs\n",
            "\n",
            "Epoch 165 Loss 0.4722 Accuracy 0.1904\n",
            "Time taken for 1 epoch: 1.4354932308197021 secs\n",
            "\n",
            "Epoch 166 Loss 0.4723 Accuracy 0.1891\n",
            "Time taken for 1 epoch: 1.3853580951690674 secs\n",
            "\n",
            "Epoch 167 Loss 0.4641 Accuracy 0.1888\n",
            "Time taken for 1 epoch: 1.4190959930419922 secs\n",
            "\n",
            "Epoch 168 Loss 0.4608 Accuracy 0.1881\n",
            "Time taken for 1 epoch: 1.4056696891784668 secs\n",
            "\n",
            "Epoch 169 Loss 0.4685 Accuracy 0.1856\n",
            "Time taken for 1 epoch: 1.3869726657867432 secs\n",
            "\n",
            "Epoch 170 Loss 0.4673 Accuracy 0.1913\n",
            "Time taken for 1 epoch: 1.410949945449829 secs\n",
            "\n",
            "Epoch 171 Loss 0.4859 Accuracy 0.1862\n",
            "Time taken for 1 epoch: 1.4021780490875244 secs\n",
            "\n",
            "Epoch 172 Loss 0.4804 Accuracy 0.1888\n",
            "Time taken for 1 epoch: 1.409513235092163 secs\n",
            "\n",
            "Epoch 173 Loss 0.4688 Accuracy 0.1872\n",
            "Time taken for 1 epoch: 1.3938899040222168 secs\n",
            "\n",
            "Epoch 174 Loss 0.4429 Accuracy 0.1978\n",
            "Time taken for 1 epoch: 1.405557632446289 secs\n",
            "\n",
            "Epoch 175 Loss 0.4760 Accuracy 0.1891\n",
            "Time taken for 1 epoch: 1.3997228145599365 secs\n",
            "\n",
            "Epoch 176 Loss 0.4734 Accuracy 0.1955\n",
            "Time taken for 1 epoch: 1.4385976791381836 secs\n",
            "\n",
            "Epoch 177 Loss 0.4518 Accuracy 0.1955\n",
            "Time taken for 1 epoch: 1.3696513175964355 secs\n",
            "\n",
            "Epoch 178 Loss 0.4324 Accuracy 0.2080\n",
            "Time taken for 1 epoch: 1.3857238292694092 secs\n",
            "\n",
            "Epoch 179 Loss 0.4525 Accuracy 0.1962\n",
            "Time taken for 1 epoch: 1.4060661792755127 secs\n",
            "\n",
            "Epoch 180 Loss 0.4251 Accuracy 0.2029\n",
            "Time taken for 1 epoch: 1.3797097206115723 secs\n",
            "\n",
            "Epoch 181 Loss 0.4419 Accuracy 0.1978\n",
            "Time taken for 1 epoch: 1.39857816696167 secs\n",
            "\n",
            "Epoch 182 Loss 0.4278 Accuracy 0.2019\n",
            "Time taken for 1 epoch: 1.4042494297027588 secs\n",
            "\n",
            "Epoch 183 Loss 0.4151 Accuracy 0.2035\n",
            "Time taken for 1 epoch: 1.3984041213989258 secs\n",
            "\n",
            "Epoch 184 Loss 0.4458 Accuracy 0.1981\n",
            "Time taken for 1 epoch: 1.4203763008117676 secs\n",
            "\n",
            "Epoch 185 Loss 0.4517 Accuracy 0.2029\n",
            "Time taken for 1 epoch: 1.3821351528167725 secs\n",
            "\n",
            "Epoch 186 Loss 0.4462 Accuracy 0.2045\n",
            "Time taken for 1 epoch: 1.4009323120117188 secs\n",
            "\n",
            "Epoch 187 Loss 0.4444 Accuracy 0.2042\n",
            "Time taken for 1 epoch: 1.3882651329040527 secs\n",
            "\n",
            "Epoch 188 Loss 0.4092 Accuracy 0.2122\n",
            "Time taken for 1 epoch: 1.3827219009399414 secs\n",
            "\n",
            "Epoch 189 Loss 0.4205 Accuracy 0.2138\n",
            "Time taken for 1 epoch: 1.4131691455841064 secs\n",
            "\n",
            "Epoch 190 Loss 0.4104 Accuracy 0.2147\n",
            "Time taken for 1 epoch: 1.4062883853912354 secs\n",
            "\n",
            "Epoch 191 Loss 0.4214 Accuracy 0.2071\n",
            "Time taken for 1 epoch: 1.412855863571167 secs\n",
            "\n",
            "Epoch 192 Loss 0.4100 Accuracy 0.2131\n",
            "Time taken for 1 epoch: 1.4127569198608398 secs\n",
            "\n",
            "Epoch 193 Loss 0.4133 Accuracy 0.2093\n",
            "Time taken for 1 epoch: 1.4162378311157227 secs\n",
            "\n",
            "Epoch 194 Loss 0.4071 Accuracy 0.2151\n",
            "Time taken for 1 epoch: 1.368819236755371 secs\n",
            "\n",
            "Epoch 195 Loss 0.4103 Accuracy 0.2109\n",
            "Time taken for 1 epoch: 1.390737533569336 secs\n",
            "\n",
            "Epoch 196 Loss 0.4159 Accuracy 0.2045\n",
            "Time taken for 1 epoch: 1.377025842666626 secs\n",
            "\n",
            "Epoch 197 Loss 0.4234 Accuracy 0.2045\n",
            "Time taken for 1 epoch: 1.3974132537841797 secs\n",
            "\n",
            "Epoch 198 Loss 0.3930 Accuracy 0.2196\n",
            "Time taken for 1 epoch: 1.3805420398712158 secs\n",
            "\n",
            "Epoch 199 Loss 0.3859 Accuracy 0.2240\n",
            "Time taken for 1 epoch: 1.4347021579742432 secs\n",
            "\n",
            "Epoch 200 Loss 0.3963 Accuracy 0.2090\n",
            "Time taken for 1 epoch: 1.3954565525054932 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2PezWhy3d1v",
        "colab_type": "text"
      },
      "source": [
        "# EVALUATION\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14QmhWesaJ2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(question):\n",
        "\n",
        "  start_token = [tokenizer.vocab_size]\n",
        "  end_token = [tokenizer.vocab_size + 1]\n",
        "  question = start_token + tokenizer.encode(question) + end_token\n",
        "  question = tf.expand_dims(question, 0)\n",
        "  answer_in = [tokenizer.vocab_size]\n",
        "  answer_in = tf.expand_dims(answer_in, 0)\n",
        "\n",
        "  for i in range(max_length_answer):\n",
        "\n",
        "    enc_padding_mask = padding_mask(question)\n",
        "    dec_padding_mask = padding_mask(answer_in)\n",
        "    dec_forward_mask = forward_mask(answer_in)\n",
        "\n",
        "    predictions = transformer(question, answer_in, training = False, enc_mask = enc_padding_mask , dec_forward_mask = dec_forward_mask, dec_padding_mask = dec_padding_mask)\n",
        "    prediction = predictions[:,-1,:] #select the last word to add to the outputs\n",
        "\n",
        "    predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
        "\n",
        "\n",
        "    #if predicted_id == end_token[0]:\n",
        "    #  return tf.squeeze(answer, axis=0)\n",
        "    predicted_id = tf.expand_dims(predicted_id,0)\n",
        "    answer_in = tf.concat([answer_in,predicted_id],axis = -1)\n",
        "  \n",
        "  return tf.squeeze(answer_in, axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqPiuGFzx6BI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  question = b'Find the first derivative of x^2 wrt x.'\n",
        "  evaluate(question)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF-YJysS62By",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  question = b'Find the first derivative of -4*a**4*v - 84*a**4 - v - 226 wrt a.'\n",
        "  start_token = [tokenizer.vocab_size]\n",
        "  end_token = [tokenizer.vocab_size + 1]\n",
        "  question = start_token + tokenizer.encode(question) + end_token\n",
        "  question = tf.expand_dims(question, 0)\n",
        "  answer_in = [tokenizer.vocab_size]\n",
        "  answer_in = tf.expand_dims(answer_in, 0)\n",
        "\n",
        "  for i in range(max_length_answer):\n",
        "\n",
        "    enc_padding_mask = padding_mask(question)\n",
        "    dec_padding_mask = padding_mask(answer_in)\n",
        "    dec_forward_mask = forward_mask(answer_in)\n",
        "    predictions = transformer(question, answer_in, training = True, enc_mask = enc_padding_mask , dec_forward_mask = dec_forward_mask, dec_padding_mask = dec_padding_mask)\n",
        "    prediction = predictions[:,-1,:] #select the last word to add to the outputs\n",
        "\n",
        "    predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
        "    predicted_id = tf.expand_dims(predicted_id,0)\n",
        "\n",
        "    #if predicted_id == end_token[0]:\n",
        "    #  break\n",
        "    answer_in = tf.concat([answer_in,predicted_id],axis = -1)\n",
        "  print(answer_in)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYmpHOETC4mS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_answer(sentence):\n",
        "  result = np.array(evaluate(sentence))\n",
        "  \n",
        "  predicted_sentence = tokenizer.decode([i for i in result \n",
        "                                            if i < tokenizer.vocab_size and i > 0])\n",
        "  print('Input: {}'.format(sentence))\n",
        "  print('Predicted answer: {}'.format(predicted_sentence))"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDG24peIC74f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0dd891f6-3892-452b-8f30-8f91a33068ad"
      },
      "source": [
        "question = b'Find the first derivative of 2*x wrt x.'\n",
        "find_answer(question)"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: b'Find the first derivative of 2*x wrt x.'\n",
            "Predicted answer:  - **oo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgYkOAK9yI_A",
        "colab_type": "text"
      },
      "source": [
        "It seems that we have complete garbage results. This tokenizer represents each number as an unique token, which doesn't seem suitable for mathematics. \n",
        "Things to try.\n",
        "\n",
        "- Test the transformer on a translation task\n",
        "- Find an input structure which understand the basic structure of a mathematical expression (trees?)"
      ]
    }
  ]
}