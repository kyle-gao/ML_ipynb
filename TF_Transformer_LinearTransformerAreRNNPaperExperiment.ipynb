{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF_Transformer_LinearTransformerAreRNNPaperExperiment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMs0D2EQk5E039yMQlAWieu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyle-gao/ML_ipynb/blob/master/TF_Transformer_LinearTransformerAreRNNPaperExperiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anQuvdlJJ8_r",
        "colab_type": "text"
      },
      "source": [
        "Copyright 2020 Yi Lin(Kyle) Gao\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_qNSzzyaCbD",
        "colab_type": "text"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOYifqcAZ03H",
        "colab_type": "text"
      },
      "source": [
        "In this notebook, I will attempt to implement the attention mechanism from the paper\n",
        "**Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention by Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, François Fleuret.** https://arxiv.org/abs/2006.16236"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "jmjh290raIky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFKnTnn7Odcp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import time"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_1FxrOUYEo9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def positional_encoding(pos, d_model):\n",
        "    \"\"\"\n",
        "    :param pos: int max position\n",
        "    :param d_model: dimension of the model\n",
        "    :return: (1,pos,d_model) array of sinusoidal positional encoding\n",
        "    \"\"\"\n",
        "    pos_enc = np.zeros((1, pos, d_model))\n",
        "    for p in range(pos):\n",
        "        for i in range(d_model // 2):\n",
        "            angles = p / np.power(10000, (2 * i) / np.float32(d_model))\n",
        "            pos_enc[:, p, 2 * i] = np.sin(angles)\n",
        "            pos_enc[:, p, 2 * i + 1] = np.cos(angles)\n",
        "        if d_model % 2 == 1:\n",
        "            # if d_model is odd loop doesn't hit last even index\n",
        "            angles = p / np.power(10000, (2 * d_model) / np.float32(d_model))\n",
        "            pos_enc[:, p, d_model - 1] = np.sin(angles)\n",
        "    return tf.cast(pos_enc, tf.float32)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7clY8qzAWi4J",
        "colab_type": "text"
      },
      "source": [
        "#Masks\n",
        "The 5d array masks are for causal attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K18W9lp1aSQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def padding_mask5(seq):\n",
        "    # Returns (batch, seq_len, 1, 1, 1 ) tensor with 0's where the sequence is padded, 1 where it is not\n",
        "\n",
        "    mask = tf.cast(tf.math.not_equal(seq,0), tf.float32)\n",
        "    #we apply mask to (m, j, h, d) <- to mask j\n",
        "    return mask[:, :, tf.newaxis, tf.newaxis,tf.newaxis]  # (batch, seq_len, 1, 1) \n",
        "\n",
        "def forward_mask5(seq):\n",
        "    \"\"\"\n",
        "    Calculates a combined forward mask and padding mask for a batch of sequences\n",
        "    :param seq: (batch,seq_len) a batch of sequences\n",
        "    :return:  a combined look_ahead_mask (lower triangular 1s)\n",
        "    and padding mask (batch, seq_len, seq_len, 1, 1)\n",
        "    \"\"\"\n",
        "    seq_len = tf.shape(seq)[1]\n",
        "\n",
        "    look_ahead_mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "    look_ahead_mask = look_ahead_mask[tf.newaxis, :, :, tf.newaxis,tf.newaxis]  # (batch, seq_len, seq_len, 1)\n",
        "\n",
        "    padded_mask = padding_mask5(seq)\n",
        "    #forward mask is applied to (m, l, j ,h) \n",
        "    #return tf.minimum(padded_mask, look_ahead_mask)\n",
        "    return padded_mask * look_ahead_mask\n",
        "\n",
        "\n",
        "def padding_mask(seq):\n",
        "    # Returns (batch, seq_len, 1, 1) tensor with 0's where the sequence is padded, 1 where it is not\n",
        "\n",
        "    mask = tf.cast(tf.math.not_equal(seq,0), tf.float32)\n",
        "    #we apply mask to (m, j, h, d) <- to mask j\n",
        "    return mask[:, :, tf.newaxis, tf.newaxis]  # (batch, seq_len, 1, 1) \n",
        "\n",
        "def forward_mask(seq):\n",
        "    \"\"\"\n",
        "    Calculates a combined forward mask and padding mask for a batch of sequences\n",
        "    :param seq: (batch,seq_len) a batch of sequences\n",
        "    :return:  a combined look_ahead_mask (lower triangular 1s)\n",
        "    and padding mask (batch, seq_len, seq_len, 1)\n",
        "    \"\"\"\n",
        "    seq_len = tf.shape(seq)[1]\n",
        "\n",
        "    look_ahead_mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "    look_ahead_mask = look_ahead_mask[tf.newaxis, :, :, tf.newaxis]  # (batch, seq_len, seq_len, 1)\n",
        "\n",
        "    padded_mask = padding_mask(seq)\n",
        "    #forward mask is applied to (m, l, j ,h) \n",
        "    #return tf.minimum(padded_mask, look_ahead_mask)\n",
        "    return padded_mask * look_ahead_mask"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRP9iT2qh4wT",
        "colab_type": "text"
      },
      "source": [
        "#Linear attention mechanism from Katharopoulos et al. \n",
        "\n",
        "Practice implementation: causal and non causal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOIdRvwHaj04",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def elu(z):\n",
        "  \"\"\"elu feature map used by Katharopoulos et al.\"\"\"\n",
        "  return tf.nn.elu(z)+1\n",
        "  \n",
        "class MultiHeadAttentionCausalMasked(tf.keras.layers.Layer):\n",
        "  \"\"\"LinearAttention Mechanism from Transformers are RNNs: Fast Autoregressive Transformers \n",
        "  with Linear Attention by Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, François Fleuret.\n",
        "\n",
        "  Uses linear feature maps f, to replace the softmax by a kernel k(x,y)->R+.\n",
        "  so that f(x)*f(y) = k(x,y)\n",
        "  \n",
        "  The authors use the elu feature map.\n",
        "  \n",
        "  This version has causal i.e. forward masking. This cannot be implemented in the usual way due to the \n",
        "  Q*K term not existing in isolation in this Linear Attention.\n",
        "  I have implemented it in clumsy way which makes this slower than the usual softmax attention by quite a bit.\n",
        "\n",
        "  Tne authors of the paper implemented causal attention via a triangular tensor product (and its back prop) in c++.\n",
        "\n",
        "  I have implemented it in clumsy way which makes this slower than the usual softmax attention by quite a bit\n",
        "  by introducing an intermediate step with the dimensions of the Q*K product. \n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,d_model,num_heads):\n",
        "    super().__init__()\n",
        "    self.d_model=d_model\n",
        "    self.num_heads=num_heads\n",
        "    \n",
        "    assert d_model%self.num_heads==0\n",
        "    \n",
        "    \n",
        "    self.depth=d_model//self.num_heads\n",
        "\n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def split_heads(self,x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads,depth)\n",
        "    Arguments:\n",
        "    x -- A tokenized sequence (batch_size,seq_len,d_model)\n",
        "    \n",
        "    Returns:\n",
        "    A tokenized sequence with dimensions (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth)) \n",
        "    return x\n",
        "\n",
        "  def call(self, q, k, v, mask=None, eps=1e-8):\n",
        "\n",
        "    batch_size = tf.shape(q)[0]\n",
        "\n",
        "    q = self.wq(q) #(batch_size,len_q, dim_q) \n",
        "    k = self.wk(k) #(batch_size,len_v, dim_q) \n",
        "    v = self.wv(v) #(batch_size,len_v, dim_v) \n",
        "\n",
        "    q = elu(self.split_heads(q, batch_size))  # (batch_size, seq_len_q, num_heads, depth_q) (m,l,h,d)\n",
        "    k = elu(self.split_heads(k, batch_size))  # (batch_size,  seq_len_v, num_heads, depth_q) (m,j,h,d)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size,  seq_len_v, num_heads, depth_v) (m,j,h,e)\n",
        "\n",
        "    k_reduced = tf.math.reduce_sum(k,axis=1) + 1e-8\n",
        "\n",
        "    z = 1/(tf.einsum(\"mlhd,mhd->mlh\", q, k_reduced)) #(batch_size, num_heads, seq_len_q)\n",
        "\n",
        "    output = tf.einsum(\"mjhd,mjhe->mjehd\",k,v) #(batch_size, len_v, depth_q, num_heads, depth_v)\n",
        "\n",
        "    output = tf.einsum(\"mlhd,mjehd,mlh->mjlhe\",q,output,z) #(batch_size, len_q, len_v, num_heads, depth_v)\n",
        "\n",
        "    if mask is not None:\n",
        "      output = output * mask #Mask must broadcast to j and l axis correctly\n",
        "    \n",
        "    output = tf.einsum(\"mjlhe->mlhe\",output) \n",
        "  \n",
        "\n",
        "    output = tf.reshape(output,(batch_size,-1,self.num_heads*self.depth)) #(batch_size,len_q, d_model)\n",
        "    return output #(m,l,h*e)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  \"\"\"Linear Attention Mechanism from Transformers are RNNs: Fast Autoregressive Transformers \n",
        "  with Linear Attention by Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, François Fleuret.\n",
        "\n",
        "  Uses linear feature maps f, to replace the softmax by a kernel k(x,y)->R+.\n",
        "  so that f(x)*f(y) = k(x,y)\n",
        "  \n",
        "  This version lacks causal masking, is quite fast.\"\"\"\n",
        "\n",
        "  def __init__(self,d_model,num_heads):\n",
        "    super().__init__()\n",
        "    self.d_model=d_model\n",
        "    self.num_heads=num_heads\n",
        "    \n",
        "    assert d_model%self.num_heads==0\n",
        "    \n",
        "    \n",
        "    self.depth=d_model//self.num_heads\n",
        "\n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def split_heads(self,x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads,depth)\n",
        "    Arguments:\n",
        "    x -- A tokenized sequence (batch_size,seq_len,d_model)\n",
        "    \n",
        "    Returns:\n",
        "    A tokenized sequence with dimensions (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth)) \n",
        "    return x\n",
        "  def __init__(self,d_model,num_heads):\n",
        "    super().__init__()\n",
        "    self.d_model=d_model\n",
        "    self.num_heads=num_heads\n",
        "    \n",
        "    assert d_model%self.num_heads==0\n",
        "    \n",
        "    \n",
        "    self.depth=d_model//self.num_heads\n",
        "\n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def split_heads(self,x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads,depth)\n",
        "    Arguments:\n",
        "    x -- A tokenized sequence (batch_size,seq_len,d_model)\n",
        "    \n",
        "    Returns:\n",
        "    A tokenized sequence with dimensions (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth)) \n",
        "    return x\n",
        "\n",
        "  def call(self,q,k,v,mask=None):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "\n",
        "    q = self.wq(q) #(batch_size,len_q, dim_q) \n",
        "    k = self.wk(k) #(batch_size,len_v, dim_q) \n",
        "    v = self.wv(v) #(batch_size,len_v, dim_v)\n",
        "\n",
        "    q = elu(self.split_heads(q, batch_size))  # (batch_size, seq_len_q, num_heads, depth_q) (m,l,h,d)\n",
        "    k = elu(self.split_heads(k, batch_size))  # (batch_size,  seq_len_k, num_heads, depth_q) (m,j,h,d)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size,  seq_len_v, num_heads, depth_v) (m,j,h,e)\n",
        "\n",
        "\n",
        "    kv = tf.einsum(\"mjhd,mjhe->mdeh\",k,v) #(batch_size, depth_k, depth_v, seq_len_v)\n",
        "\n",
        "    if mask is not None: #padding mask is (m,j,1,1)\n",
        "                          #causal mask is (m,j,j,1) cannot be broadcast here\n",
        "       k = k*mask\n",
        "\n",
        "    #we contract k over the j axis and add an epsilon numerical stability.\n",
        "    k_reduced = tf.math.reduce_sum(k,axis=1) + 1e-8\n",
        "\n",
        "    z = 1/(tf.einsum(\"mlhd,mhd->mlh\", q, k_reduced)) #(batch_size, num_heads, seq_len_q)\n",
        "\n",
        "    output = tf.einsum(\"mlhd,mdeh,mlh->mlhe\",q,kv,z) #(batch_size,len_q, heads, depth_v)\n",
        "    output = tf.reshape(output,(batch_size,-1,self.num_heads*self.depth)) #(batch_size,len_q, d_model)\n",
        "\n",
        "    return output"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwFRZhdwWwK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"The EncoderLayer consists of one MultiHeadAttention layer connected to a FeedForward layer,\n",
        "    each of these 2 layers have a residual connection.\"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, d_model, dense_dim, dropout=0.1, causal = False):\n",
        "        super().__init__()\n",
        "        if causal: \n",
        "          self.attention = MultiHeadAttentionCausalMasked(d_model, num_heads)         \n",
        "        else:\n",
        "          self.attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.dense = tf.keras.Sequential([tf.keras.layers.Dense(dense_dim, activation='relu'),\n",
        "                                          tf.keras.layers.Dense(d_model)])\n",
        "\n",
        "        self.norm1 = tf.keras.layers.LayerNormalization()\n",
        "        self.norm2 = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(dropout)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        out_attention = self.attention(x, x, x, mask)  # (batch_size,seq_len,d_model)\n",
        "        out_attention = self.dropout1(out_attention, training=training)\n",
        "        out1 = self.norm1(x + out_attention)  # residual connection (batch_size,seq_len,d_model)\n",
        "\n",
        "        out_dense = self.dense(out1)  # (batch_size,seq_len,d_model)\n",
        "        out2 = self.norm2(out1 + out_dense)  # residual conenction (batch_size,seq_len,d_model)\n",
        "        return out2\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    \"\"\"The Encoder consists of EncoderLayer\"\"\"\n",
        "\n",
        "    def __init__(self, num_layers, num_heads, d_model, dense_dim,\n",
        "                 vocab_size, max_encoding_position, dropout=0.1, causal = False):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = positional_encoding(max_encoding_position, d_model)\n",
        "        self.encoding_layers = [EncoderLayer(num_heads, d_model, dense_dim, dropout, causal= causal) for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x, training, mask=None):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        x = self.embedding(x)  # (batch_size,input_len,d_model)\n",
        "        x = x * tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x = x + self.positional_encoding[:, :seq_len, :]\n",
        "        x = self.dropout(x, training=training)\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.encoding_layers[i](x, training, mask)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return x\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"A decoder layers consists of two MultiHeadAttention, one for the Decoder input, one from Encoder output\"\"\"\n",
        "    def __init__(self, num_heads, d_model, dense_dim, dropout=0.1,causal=False):\n",
        "        super().__init__()\n",
        "        if causal:\n",
        "          self.attention1 = MultiHeadAttentionCausalMasked(d_model, num_heads)\n",
        "          self.attention2 = MultiHeadAttentionCausalMasked(d_model, num_heads)         \n",
        "        else:\n",
        "          self.attention1 = MultiHeadAttention(d_model, num_heads)\n",
        "          self.attention2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.dense = tf.keras.Sequential([tf.keras.layers.Dense(dense_dim, activation='relu'),\n",
        "                                          tf.keras.layers.Dense(d_model)])\n",
        "\n",
        "        self.norm1 = tf.keras.layers.LayerNormalization()\n",
        "        self.norm2 = tf.keras.layers.LayerNormalization()\n",
        "        self.norm3 = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(dropout)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(dropout)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, encoder_out, x, training, forward_mask, padding_mask, ):\n",
        "\n",
        "        out_attention1 = self.attention1(x, x, x,\n",
        "                                         mask = forward_mask)  # (batch_size, seq_len_answer, d_model) -> The return seq_len is the same as that of the first argument of the call.\n",
        "        out_attention1 = self.dropout1(out_attention1, training=training)\n",
        "        out1 = self.norm1(x + out_attention1)  # residual connection (batch_size, seq_len_answer, d_model)\n",
        "\n",
        "        out_attention2 = self.attention2(out1, encoder_out, encoder_out,\n",
        "                                         padding_mask)  # (batch_size, seq_len_answer, d_model)\n",
        "        out_attention2 = self.dropout2(out_attention2, training=training)\n",
        "        out2 = self.norm2(out1 + out_attention2)\n",
        "\n",
        "        out_dense = self.dense(out2)\n",
        "        out_dense = self.dropout3(out_dense + out2)\n",
        "\n",
        "        return out_dense\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    \"\"\"The Decoder consists of multiple DecoderLayer\"\"\"\n",
        "    def __init__(self, num_layers, num_heads, d_model, dense_dim,\n",
        "                 vocab_size, max_encoding_position, dropout=0.1, causal = False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = positional_encoding(max_encoding_position, d_model)\n",
        "        self.decoder_layers = [DecoderLayer(num_heads, d_model, dense_dim, dropout, causal = causal) for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, encoder_out, x, training, forward_mask=None, padding_mask=None):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        x = self.embedding(x)  # (batch_size,input_len,d_model)\n",
        "        x = x * tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x = x + self.positional_encoding[:, :seq_len, :]\n",
        "        x = self.dropout(x, training=training)\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.decoder_layers[i](encoder_out, x, training, forward_mask,\n",
        "                                       padding_mask)  # (batch_size, input_seq_len, d_model)\n",
        "        return x\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, num_layers, num_heads, d_model, dense_dim, in_vocab_size, tar_vocab_size,\n",
        "                 input_max_position, target_max_position, rate=0.1, causal = False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, num_heads, d_model, dense_dim,\n",
        "                               in_vocab_size, max_encoding_position=input_max_position, dropout=0.1, causal=causal)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, num_heads, d_model, dense_dim,\n",
        "                               tar_vocab_size, max_encoding_position=target_max_position, dropout=0.1,causal=causal)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(tar_vocab_size)\n",
        "\n",
        "    def call(self, input, target, training=False, enc_mask=None, dec_forward_mask=None, dec_padding_mask=None):\n",
        "        out_encoder = self.encoder(input, training=training, mask=enc_mask)\n",
        "\n",
        "        out_decoder = self.decoder(out_encoder, target, training=training, forward_mask=dec_forward_mask,\n",
        "                                   padding_mask=dec_padding_mask)\n",
        "\n",
        "        out = self.dense(out_decoder)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY9NQjtiaiG0",
        "colab_type": "text"
      },
      "source": [
        "#Training and Evaluation\n",
        "\n",
        "Data and preprocessing from Trung Tran\n",
        "https://machinetalk.org/2019/04/29/create-the-transformer-with-tensorflow-2-0/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifijMb7aakO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_data = (\n",
        "    ('What a ridiculous concept!', 'Quel concept ridicule !'),\n",
        "    ('Your idea is not entirely crazy.', \"Votre idée n'est pas complètement folle.\"),\n",
        "    (\"A man's worth lies in what he is.\", \"La valeur d'un homme réside dans ce qu'il est.\"),\n",
        "    ('What he did is very wrong.', \"Ce qu'il a fait est très mal.\"),\n",
        "    (\"All three of you need to do that.\", \"Vous avez besoin de faire cela, tous les trois.\"),\n",
        "    (\"Are you giving me another chance?\", \"Me donnez-vous une autre chance ?\"),\n",
        "    (\"Both Tom and Mary work as models.\", \"Tom et Mary travaillent tous les deux comme mannequins.\"),\n",
        "    (\"Can I have a few minutes, please?\", \"Puis-je avoir quelques minutes, je vous prie ?\"),\n",
        "    (\"Could you close the door, please?\", \"Pourriez-vous fermer la porte, s'il vous plaît ?\"),\n",
        "    (\"Did you plant pumpkins this year?\", \"Cette année, avez-vous planté des citrouilles ?\"),\n",
        "    (\"Do you ever study in the library?\", \"Est-ce que vous étudiez à la bibliothèque des fois ?\"),\n",
        "    (\"Don't be deceived by appearances.\", \"Ne vous laissez pas abuser par les apparences.\"),\n",
        "    (\"Excuse me. Can you speak English?\", \"Je vous prie de m'excuser ! Savez-vous parler anglais ?\"),\n",
        "    (\"Few people know the true meaning.\", \"Peu de gens savent ce que cela veut réellement dire.\"),\n",
        "    (\"Germany produced many scientists.\", \"L'Allemagne a produit beaucoup de scientifiques.\"),\n",
        "    (\"Guess whose birthday it is today.\", \"Devine de qui c'est l'anniversaire, aujourd'hui !\"),\n",
        "    (\"He acted like he owned the place.\", \"Il s'est comporté comme s'il possédait l'endroit.\"),\n",
        "    (\"Honesty will pay in the long run.\", \"L'honnêteté paye à la longue.\"),\n",
        "    (\"How do we know this isn't a trap?\", \"Comment savez-vous qu'il ne s'agit pas d'un piège ?\"),\n",
        "    (\"I can't believe you're giving up.\", \"Je n'arrive pas à croire que vous abandonniez.\"),\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX2_2JFKalj8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import unicodedata\n",
        "import re\n",
        "\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def normalize_string(s):\n",
        "    s = unicode_to_ascii(s)\n",
        "    s = re.sub(r'([!.?])', r' \\1', s)\n",
        "    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
        "    s = re.sub(r'\\s+', r' ', s)\n",
        "    return s\n",
        "\n",
        "raw_data_en, raw_data_fr = list(zip(*raw_data))\n",
        "raw_data_en, raw_data_fr = list(raw_data_en), list(raw_data_fr)\n",
        "raw_data_en = ['<start> ' + normalize_string(data) + ' <end>' for data in raw_data_en]\n",
        "raw_data_fr = ['<start> ' + normalize_string(data) + ' <end>' for data in raw_data_fr]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNLysa7Ba44E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_data_en=[s.lower() for s in raw_data_en]\n",
        "raw_data_fr=[s.lower() for s in raw_data_fr]\n",
        "\n",
        "tokenizer_en = tf.keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n',lower=False)\n",
        "tokenizer_en.fit_on_texts(raw_data_en)\n",
        "data_en=tokenizer_en.texts_to_sequences(raw_data_en)\n",
        "data_en=tf.keras.preprocessing.sequence.pad_sequences(data_en,padding=\"post\")\n",
        "\n",
        "tokenizer_fr = tf.keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n',lower=False)\n",
        "tokenizer_fr.fit_on_texts(raw_data_fr)\n",
        "data_fr_in = tokenizer_fr.texts_to_sequences(raw_data_fr)\n",
        "data_fr_in = tf.keras.preprocessing.sequence.pad_sequences(data_fr_in,\n",
        "                                                           padding='post')\n",
        "\n",
        "BATCH_SIZE = 20\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (data_fr_in, data_en))\n",
        "small_dataset = dataset.shuffle(20).batch(BATCH_SIZE).cache()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDNlDFgweof_",
        "colab_type": "text"
      },
      "source": [
        "Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQM8wZPLcHqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_layers = 2\n",
        "d_model = 128\n",
        "dense_dim = 512\n",
        "num_heads = 2\n",
        "max_len_fr = 35\n",
        "max_len_en = 35\n",
        "en_vocab_size = len(tokenizer_en.word_index) +2\n",
        "fr_vocab_size = len(tokenizer_fr.word_index) + 2 \n",
        "dropout_rate = 0.1\n",
        "causal = False"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StUqOk4seqLe",
        "colab_type": "text"
      },
      "source": [
        "Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQlMu9Z_WzpI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(question):\n",
        "\n",
        "    start_token = [1]\n",
        "    end_token = [2]\n",
        "    question = tokenizer_fr.texts_to_sequences([question])[0]\n",
        "    question = tf.expand_dims(question, 0)\n",
        "    answer_in = start_token\n",
        "    answer_in = tf.expand_dims(answer_in, 0)\n",
        "\n",
        "    for i in range(max_len_fr):\n",
        "        if causal:\n",
        "          enc_padding_mask = padding_mask5(question)\n",
        "          dec_padding_mask = padding_mask5(question)\n",
        "          dec_forward_mask = forward_mask5(answer_in)\n",
        "        else:\n",
        "          enc_padding_mask = padding_mask(question)\n",
        "          dec_padding_mask = padding_mask(question)\n",
        "          dec_forward_mask = padding_mask(answer_in)\n",
        "\n",
        "        predictions = transformer(question, answer_in, training=False, enc_mask=enc_padding_mask,\n",
        "                                  dec_forward_mask=dec_forward_mask, dec_padding_mask=dec_padding_mask)\n",
        "        prediction = predictions[:, -1, :]  # select the last word to add to the outputs\n",
        "\n",
        "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
        "\n",
        "        if predicted_id == 2:\n",
        "            return tf.squeeze(answer_in, axis=0)\n",
        "        predicted_id = tf.expand_dims(predicted_id, 0)\n",
        "        answer_in = tf.concat([answer_in, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(answer_in, axis=0)\n",
        "\n",
        "def translate(sentence):\n",
        "  result = [np.array(evaluate(sentence))]\n",
        "  \n",
        "  predicted_sentence = tokenizer_en.sequences_to_texts(result)\n",
        "  print('Input: {}'.format(sentence))\n",
        "  print('Predicted translation: {}'.format(predicted_sentence))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uS_YcRI6esLH",
        "colab_type": "text"
      },
      "source": [
        "Loss function and Optimzer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABu70dFYXSUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(0.0003, beta_1=0.9, beta_2=0.98,\n",
        "                                         epsilon=1e-9)\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "\n",
        "def masked_loss_fn(answer, prediction):\n",
        "        mask = tf.math.logical_not(tf.math.equal(answer, 0))  # 0 at zeroes, 1 at non-zeroes since seq is padded\n",
        "        # mask = tf.math.equal(answer, 0)\n",
        "        mask = tf.cast(mask, tf.int32)\n",
        "        loss_value = loss_fn(answer, prediction,\n",
        "                             sample_weight=mask)  # set the zeros to zero weight, other values have weight of 1.\n",
        "\n",
        "        return loss_value\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "        name='train_accuracy')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZyJPJQMcpyT",
        "colab_type": "text"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCEAVQCJexId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "causal = False\n",
        "transformer = Transformer(num_layers=num_layers, num_heads=num_heads, d_model=d_model, dense_dim=dense_dim,\n",
        "                          in_vocab_size=fr_vocab_size, tar_vocab_size=en_vocab_size,\n",
        "                          input_max_position=max_len_fr, target_max_position=max_len_en, rate=0.1, causal = causal)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OryxpF_Xa11",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 926
        },
        "outputId": "4ef60dff-6061-4513-8033-8a8220b03059"
      },
      "source": [
        "    signature = [tf.TensorSpec(shape=(None,None), dtype=tf.int32),\n",
        "                 tf.TensorSpec(shape=(None,None),\n",
        "                               dtype=tf.int32), ]  # a bit faster if we specify the signature\n",
        "    EPOCHS = 200\n",
        "    @tf.function(input_signature=signature)\n",
        "    def train_step(question, answer):\n",
        "        answer_in = answer[:, :-1]\n",
        "        answer_tar = answer[:, 1:]\n",
        "\n",
        "        if causal:\n",
        "          enc_padding_mask = padding_mask5(question)\n",
        "          dec_padding_mask = padding_mask5(question)\n",
        "          dec_forward_mask = forward_mask5(answer_in)\n",
        "        else:\n",
        "          enc_padding_mask = padding_mask(question)\n",
        "          dec_padding_mask = padding_mask(question)\n",
        "          dec_forward_mask = padding_mask(answer_in)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = transformer(question, answer_in, training=True, enc_mask=enc_padding_mask,\n",
        "                                      dec_forward_mask=dec_forward_mask, dec_padding_mask=dec_padding_mask)\n",
        "            loss = masked_loss_fn(answer_tar, predictions)\n",
        "\n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "        train_loss(loss)\n",
        "        train_accuracy(answer_tar, predictions)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        start = time.time()\n",
        "\n",
        "        train_loss.reset_states()\n",
        "        train_accuracy.reset_states()\n",
        "\n",
        "        for (batch, (question, answer)) in enumerate(small_dataset):\n",
        "            train_step(question, answer)\n",
        "        if (epoch +1) % 20 == 0:\n",
        "          print('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1,\n",
        "                                                            train_loss.result(),\n",
        "                                                            train_accuracy.result()))\n",
        "        \n",
        "          print('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n",
        "          id = np.random.randint(0,20)\n",
        "          translate(raw_data_fr[id])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 20 Loss 3.0401 Accuracy 0.1000\n",
            "Time taken for 1 epoch: 0.018494844436645508 secs\n",
            "\n",
            "Input: <start> est ce que vous etudiez a la bibliotheque des fois ? <end>\n",
            "Predicted translation: ['<start>']\n",
            "Epoch 40 Loss 1.6015 Accuracy 0.3500\n",
            "Time taken for 1 epoch: 0.017268896102905273 secs\n",
            "\n",
            "Input: <start> ne vous laissez pas abuser par les apparences . <end>\n",
            "Predicted translation: ['<start> do do do do do do do']\n",
            "Epoch 60 Loss 0.5577 Accuracy 0.6700\n",
            "Time taken for 1 epoch: 0.01831960678100586 secs\n",
            "\n",
            "Input: <start> tom et mary travaillent tous les deux comme mannequins . <end>\n",
            "Predicted translation: ['<start> all three of all three of all three of']\n",
            "Epoch 80 Loss 0.1778 Accuracy 0.7400\n",
            "Time taken for 1 epoch: 0.021340370178222656 secs\n",
            "\n",
            "Input: <start> me donnez vous une autre chance ? <end>\n",
            "Predicted translation: ['<start> all three of you ever another chance']\n",
            "Epoch 100 Loss 0.1047 Accuracy 0.7500\n",
            "Time taken for 1 epoch: 0.018093585968017578 secs\n",
            "\n",
            "Input: <start> l honnetete paye a la longue . <end>\n",
            "Predicted translation: ['<start> all three of you need to do that']\n",
            "Epoch 120 Loss 0.0454 Accuracy 0.7550\n",
            "Time taken for 1 epoch: 0.018412113189697266 secs\n",
            "\n",
            "Input: <start> il s est comporte comme s il possedait l endroit . <end>\n",
            "Predicted translation: ['<start> he acted like he owned the place']\n",
            "Epoch 140 Loss 0.0361 Accuracy 0.7500\n",
            "Time taken for 1 epoch: 0.017638444900512695 secs\n",
            "\n",
            "Input: <start> ce qu il a fait est tres mal . <end>\n",
            "Predicted translation: ['<start> a man s worth lies in what he is']\n",
            "Epoch 160 Loss 0.0240 Accuracy 0.7550\n",
            "Time taken for 1 epoch: 0.018506526947021484 secs\n",
            "\n",
            "Input: <start> ce qu il a fait est tres mal . <end>\n",
            "Predicted translation: ['<start> a man s worth lies in what he is']\n",
            "Epoch 180 Loss 0.0178 Accuracy 0.7550\n",
            "Time taken for 1 epoch: 0.01823902130126953 secs\n",
            "\n",
            "Input: <start> je n arrive pas a croire que vous abandonniez . <end>\n",
            "Predicted translation: ['<start> i can t believe you re giving up']\n",
            "Epoch 200 Loss 0.0137 Accuracy 0.7550\n",
            "Time taken for 1 epoch: 0.017964601516723633 secs\n",
            "\n",
            "Input: <start> vous avez besoin de faire cela tous les trois . <end>\n",
            "Predicted translation: ['<start> all three of you need to do that']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTVLp_yijhrI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7b4dffd4-05cd-4c39-a152-acd34d88fbca"
      },
      "source": [
        "for fr in raw_data_fr:\n",
        "  translate(fr)\n",
        "  print(\"\\n\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> quel concept ridicule ! <end>\n",
            "Predicted translation: ['<start> a few minutes please']\n",
            "\n",
            "\n",
            "Input: <start> votre idee n est pas completement folle . <end>\n",
            "Predicted translation: ['<start> all three of you need to do that']\n",
            "\n",
            "\n",
            "Input: <start> la valeur d un homme reside dans ce qu il est . <end>\n",
            "Predicted translation: ['<start> a man s worth lies in what he is']\n",
            "\n",
            "\n",
            "Input: <start> ce qu il a fait est tres mal . <end>\n",
            "Predicted translation: ['<start> a man s worth lies in what he is']\n",
            "\n",
            "\n",
            "Input: <start> vous avez besoin de faire cela tous les trois . <end>\n",
            "Predicted translation: ['<start> all three of you need to do that']\n",
            "\n",
            "\n",
            "Input: <start> me donnez vous une autre chance ? <end>\n",
            "Predicted translation: ['<start> all three of you need to do that']\n",
            "\n",
            "\n",
            "Input: <start> tom et mary travaillent tous les deux comme mannequins . <end>\n",
            "Predicted translation: ['<start> both tom and mary work as models']\n",
            "\n",
            "\n",
            "Input: <start> puis je avoir quelques minutes je vous prie ? <end>\n",
            "Predicted translation: ['<start> can i can i have a few minutes please']\n",
            "\n",
            "\n",
            "Input: <start> pourriez vous fermer la porte s il vous plait ? <end>\n",
            "Predicted translation: ['<start> could you close the door please']\n",
            "\n",
            "\n",
            "Input: <start> cette annee avez vous plante des citrouilles ? <end>\n",
            "Predicted translation: ['<start> all three of you need to do that']\n",
            "\n",
            "\n",
            "Input: <start> est ce que vous etudiez a la bibliotheque des fois ? <end>\n",
            "Predicted translation: ['<start> do we know this isn t a trap']\n",
            "\n",
            "\n",
            "Input: <start> ne vous laissez pas abuser par les apparences . <end>\n",
            "Predicted translation: ['<start> don t be deceived by appearances']\n",
            "\n",
            "\n",
            "Input: <start> je vous prie de m excuser ! savez vous parler anglais ? <end>\n",
            "Predicted translation: ['<start> excuse me can you speak english']\n",
            "\n",
            "\n",
            "Input: <start> peu de gens savent ce que cela veut reellement dire . <end>\n",
            "Predicted translation: ['<start> few people know the true meaning']\n",
            "\n",
            "\n",
            "Input: <start> l allemagne a produit beaucoup de scientifiques . <end>\n",
            "Predicted translation: ['<start> all three of you need to do that']\n",
            "\n",
            "\n",
            "Input: <start> devine de qui c est l anniversaire aujourd hui ! <end>\n",
            "Predicted translation: ['<start> a man s worth lies in what he is']\n",
            "\n",
            "\n",
            "Input: <start> il s est comporte comme s il possedait l endroit . <end>\n",
            "Predicted translation: ['<start> he acted like he owned the place']\n",
            "\n",
            "\n",
            "Input: <start> l honnetete paye a la longue . <end>\n",
            "Predicted translation: ['<start> all three of you need to do that']\n",
            "\n",
            "\n",
            "Input: <start> comment savez vous qu il ne s agit pas d un piege ? <end>\n",
            "Predicted translation: ['<start> a trap a man s worth lies in what he is']\n",
            "\n",
            "\n",
            "Input: <start> je n arrive pas a croire que vous abandonniez . <end>\n",
            "Predicted translation: ['<start> i can t believe you re giving up']\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-pUln2efL7K",
        "colab_type": "text"
      },
      "source": [
        "#With causal masking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz9-KImTfNjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "causal = True\n",
        "transformer = Transformer(num_layers=num_layers, num_heads=num_heads, d_model=d_model, dense_dim=dense_dim,\n",
        "                          in_vocab_size=fr_vocab_size, tar_vocab_size=en_vocab_size,\n",
        "                          input_max_position=max_len_fr, target_max_position=max_len_en, rate=0.1, causal = causal)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxrD-6c-coAd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 926
        },
        "outputId": "0163e283-9200-41b6-f57e-eb35bd040f35"
      },
      "source": [
        "    signature = [tf.TensorSpec(shape=(None,None), dtype=tf.int32),\n",
        "                 tf.TensorSpec(shape=(None,None),\n",
        "                               dtype=tf.int32), ]  # a bit faster if we specify the signature\n",
        "    EPOCHS = 200\n",
        "    @tf.function(input_signature=signature)\n",
        "    def train_step(question, answer):\n",
        "        answer_in = answer[:, :-1]\n",
        "        answer_tar = answer[:, 1:]\n",
        "        if causal:\n",
        "          enc_padding_mask = padding_mask5(question)\n",
        "          dec_padding_mask = padding_mask5(question)\n",
        "          dec_forward_mask = forward_mask5(answer_in)\n",
        "        else:\n",
        "          enc_padding_mask = padding_mask(question)\n",
        "          dec_padding_mask = padding_mask(question)\n",
        "          dec_forward_mask = padding_mask(answer_in)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = transformer(question, answer_in, training=True, enc_mask=enc_padding_mask,\n",
        "                                      dec_forward_mask=dec_forward_mask, dec_padding_mask=dec_padding_mask)\n",
        "            loss = masked_loss_fn(answer_tar, predictions)\n",
        "\n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "        train_loss(loss)\n",
        "        train_accuracy(answer_tar, predictions)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        start = time.time()\n",
        "\n",
        "        train_loss.reset_states()\n",
        "        train_accuracy.reset_states()\n",
        "\n",
        "        for (batch, (question, answer)) in enumerate(small_dataset):\n",
        "            train_step(question, answer)\n",
        "        if (1+ epoch) % 20 == 0:\n",
        "          print('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1,\n",
        "                                                            train_loss.result(),\n",
        "                                                            train_accuracy.result()))\n",
        "        \n",
        "          print('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n",
        "          id = np.random.randint(0,20)\n",
        "          translate(raw_data_fr[id])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 20 Loss 1.9159 Accuracy 0.3000\n",
            "Time taken for 1 epoch: 0.0251312255859375 secs\n",
            "\n",
            "Input: <start> je vous prie de m excuser ! savez vous parler anglais ? <end>\n",
            "Predicted translation: ['<start> can t a few the english']\n",
            "Epoch 40 Loss 0.5980 Accuracy 0.6800\n",
            "Time taken for 1 epoch: 0.025606632232666016 secs\n",
            "\n",
            "Input: <start> la valeur d un homme reside dans ce qu il est . <end>\n",
            "Predicted translation: ['<start> a a man s worth lies in']\n",
            "Epoch 60 Loss 0.1684 Accuracy 0.7350\n",
            "Time taken for 1 epoch: 0.028249025344848633 secs\n",
            "\n",
            "Input: <start> quel concept ridicule ! <end>\n",
            "Predicted translation: ['<start> what a ridiculous concept']\n",
            "Epoch 80 Loss 0.0713 Accuracy 0.7550\n",
            "Time taken for 1 epoch: 0.0248415470123291 secs\n",
            "\n",
            "Input: <start> je vous prie de m excuser ! savez vous parler anglais ? <end>\n",
            "Predicted translation: ['<start> excuse me can you speak english']\n",
            "Epoch 100 Loss 0.0427 Accuracy 0.7550\n",
            "Time taken for 1 epoch: 0.025027990341186523 secs\n",
            "\n",
            "Input: <start> devine de qui c est l anniversaire aujourd hui ! <end>\n",
            "Predicted translation: ['<start> guess whose birthday it is today']\n",
            "Epoch 120 Loss 0.0307 Accuracy 0.7500\n",
            "Time taken for 1 epoch: 0.0250856876373291 secs\n",
            "\n",
            "Input: <start> est ce que vous etudiez a la bibliotheque des fois ? <end>\n",
            "Predicted translation: ['<start> do you ever study in the library']\n",
            "Epoch 140 Loss 0.0189 Accuracy 0.7550\n",
            "Time taken for 1 epoch: 0.03295540809631348 secs\n",
            "\n",
            "Input: <start> puis je avoir quelques minutes je vous prie ? <end>\n",
            "Predicted translation: ['<start> can i have a few minutes please']\n",
            "Epoch 160 Loss 0.0162 Accuracy 0.7550\n",
            "Time taken for 1 epoch: 0.026874542236328125 secs\n",
            "\n",
            "Input: <start> me donnez vous une autre chance ? <end>\n",
            "Predicted translation: ['<start> are you giving me another chance']\n",
            "Epoch 180 Loss 0.0161 Accuracy 0.7550\n",
            "Time taken for 1 epoch: 0.02514791488647461 secs\n",
            "\n",
            "Input: <start> vous avez besoin de faire cela tous les trois . <end>\n",
            "Predicted translation: ['<start> all three of you need to do that']\n",
            "Epoch 200 Loss 0.0088 Accuracy 0.7550\n",
            "Time taken for 1 epoch: 0.02599358558654785 secs\n",
            "\n",
            "Input: <start> cette annee avez vous plante des citrouilles ? <end>\n",
            "Predicted translation: ['<start> did you plant pumpkins this year']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13eBQrg8gzJg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a05c3175-5791-42b8-ec23-9cdd2b2591a9"
      },
      "source": [
        "for fr in raw_data_fr:\n",
        "  translate(fr)\n",
        "  print(\"\\n\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> quel concept ridicule ! <end>\n",
            "Predicted translation: ['<start> what a ridiculous concept']\n",
            "\n",
            "\n",
            "Input: <start> votre idee n est pas completement folle . <end>\n",
            "Predicted translation: ['<start> your idea is not entirely crazy']\n",
            "\n",
            "\n",
            "Input: <start> la valeur d un homme reside dans ce qu il est . <end>\n",
            "Predicted translation: ['<start> a man s worth lies in what he is']\n",
            "\n",
            "\n",
            "Input: <start> ce qu il a fait est tres mal . <end>\n",
            "Predicted translation: ['<start> what he did is very wrong']\n",
            "\n",
            "\n",
            "Input: <start> vous avez besoin de faire cela tous les trois . <end>\n",
            "Predicted translation: ['<start> all three of you need to do that']\n",
            "\n",
            "\n",
            "Input: <start> me donnez vous une autre chance ? <end>\n",
            "Predicted translation: ['<start> are you giving me another chance']\n",
            "\n",
            "\n",
            "Input: <start> tom et mary travaillent tous les deux comme mannequins . <end>\n",
            "Predicted translation: ['<start> how do we know the library']\n",
            "\n",
            "\n",
            "Input: <start> puis je avoir quelques minutes je vous prie ? <end>\n",
            "Predicted translation: ['<start> can i have a few minutes please']\n",
            "\n",
            "\n",
            "Input: <start> pourriez vous fermer la porte s il vous plait ? <end>\n",
            "Predicted translation: ['<start> could you close the door please']\n",
            "\n",
            "\n",
            "Input: <start> cette annee avez vous plante des citrouilles ? <end>\n",
            "Predicted translation: ['<start> did you plant pumpkins this year']\n",
            "\n",
            "\n",
            "Input: <start> est ce que vous etudiez a la bibliotheque des fois ? <end>\n",
            "Predicted translation: ['<start> do you ever study in the library']\n",
            "\n",
            "\n",
            "Input: <start> ne vous laissez pas abuser par les apparences . <end>\n",
            "Predicted translation: ['<start> don t be deceived by appearances']\n",
            "\n",
            "\n",
            "Input: <start> je vous prie de m excuser ! savez vous parler anglais ? <end>\n",
            "Predicted translation: ['<start> excuse me can you speak english']\n",
            "\n",
            "\n",
            "Input: <start> peu de gens savent ce que cela veut reellement dire . <end>\n",
            "Predicted translation: ['<start> few people know the true meaning']\n",
            "\n",
            "\n",
            "Input: <start> l allemagne a produit beaucoup de scientifiques . <end>\n",
            "Predicted translation: ['<start> germany produced many scientists']\n",
            "\n",
            "\n",
            "Input: <start> devine de qui c est l anniversaire aujourd hui ! <end>\n",
            "Predicted translation: ['<start> guess whose birthday it is today']\n",
            "\n",
            "\n",
            "Input: <start> il s est comporte comme s il possedait l endroit . <end>\n",
            "Predicted translation: ['<start> he acted like he owned the place']\n",
            "\n",
            "\n",
            "Input: <start> l honnetete paye a la longue . <end>\n",
            "Predicted translation: ['<start> honesty will pay in the long run']\n",
            "\n",
            "\n",
            "Input: <start> comment savez vous qu il ne s agit pas d un piege ? <end>\n",
            "Predicted translation: ['<start> how do we know this isn t a trap']\n",
            "\n",
            "\n",
            "Input: <start> je n arrive pas a croire que vous abandonniez . <end>\n",
            "Predicted translation: ['<start> t believe you re giving up']\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ3F744ijhLb",
        "colab_type": "text"
      },
      "source": [
        "From a quick inspection we see that the use of causal masking improves the model's performance.\n",
        "\n",
        "**Input: <start> quel concept ridicule ! <end>**\n",
        "\n",
        "Predicted translation: ['<start> a few minutes please'] #no causal\n",
        "\n",
        "\n",
        "Predicted translation: ['<start> what a ridiculous concept'] #causal (correct)\n",
        "\n",
        "**Input: <start> puis je avoir quelques minutes je vous prie ? <end>**\n",
        "\n",
        "\n",
        "Predicted translation: ['<start> can i can i have a few minutes please']# no causal\n",
        "\n",
        "\n",
        "Predicted translation: ['<start> can i have a few minutes please'] #causal (correct)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy3dM3helMAJ",
        "colab_type": "text"
      },
      "source": [
        "Both model managed to fit(overfit) the training data.\n",
        "\n",
        "The training speed are similar for this small dataset. However they are vastly different for larger datasets, with my implementation of causal masking being significantely slower (to the point of being untrainable on my machine).\n",
        "\n",
        "A future test could be to verify the linearity wrt. sequence length;\n",
        "\n",
        "Is the non causal attention mechanism linear wtr. sequence length?\n"
      ]
    }
  ]
}